# 爬虫

## 第一章 基本库的使用

### Requests

#### 1.抓取网页

- 抓取知乎首页


- 利用正则匹配所有问题的标题

  ```
  import requests
  import re

  url = 'https://www.zhihu.com/explore'
  headers  = {
  'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
  }
  r =requests.get(url=url, headers=headers)
  pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
  titles = re.findall(pattern, r.text)
  print(titles)
  ```

  ​

#### 2.抓取二进制数据

- 抓取GitHub的图标icon

- 使用python IO流保存icon文件

- wb ：二进制打开文件

  ```
  import requests

  r = requests.get("https://github.com/favicon.ico")
  with open('favicon.ico', 'wb) as f:
  	f.write(r.content)
  ```


#### 3.Response

- requests内置status code可以用来判断请求是否成功

  ```
  import requests

  r = requests.get('http://www.jianshu.com')
  # 判断请求是否成功
  # 如果返回码
  exit() if not r.status_code == requests.codes.ok else print('Requests Fail')
  ```



#### 4.文件上传

- requests模拟提交数据

- 文件上传会单独有一个files字段来标识

  ```
  import requests

  file = {'file': open('favicon.ico', 'rb')}
  r = requests.post('http://httpbin.org/post', files=file)
  print(r.text)
  ```

  ​

#### 5.会话保持

- 维持同一个会话

- 定义session后再调用

  ```
  import requests

  s = requests.Session()    # 会话保持
  s.get('xxxxx')
  ```

#### 6.SSL证书验证

- 关闭证书验证

- 忽略证书警告

  ```
  import requests
  from requests.packages import urllib3

  # 忽略警告
  urllib3.disable_warnings()

  r = requests.get('https://www.12306.cn', verify=False)
  print(r.status_code)
  ```


- 捕捉警告到日志来忽略警告

  ```
  import requests
  import logging

  #捕捉警告道日志  忽略警告
  logging.captureWarnings(True)

  r = requests.get('https://www.12306.cn', verify=False)
  print(r.status_code)
  ```

#### 7.超时设置

- 超出时间没有响应就报错

- 超时单位：s

- 请求分为两个阶段：connect、read，所以可以传入元祖分别指定这两个时间

  ```
  import requests

  # 超时设置，单位s
  r = requests.get('http://www.taobao.com', timeout=1)

  # 超时设置，传入元祖指定connect、read
  r = requests.get('http://www.taobao.com', timeout=(5,11))
  ```


- 永久等待，不设置默认就是none

#### 8.身份认证

- requests自带的身份认证功能

- 认证成功返回200，失败则返回401

  ```
  import requests
  from requests.auth import HTTPBasicAuth

  r = requests.get('xxxx', auth=HTTPBasicAuth('username', 'password'))
  ```


- 简写版，不用导入HTTPBasicAuth，直接传元祖，默认使用这个类认证

  ```
  import requests
  r = requests.get('xxxx', auth=('username', 'password'))
  ```

#### 9.Requests对象

- requests各个参数可以通过一个requests对象表示

- 调用prepare_requests方法转换成对象

- 使用send方法发送对象

- 构造对象，方便进行**队列调度**

  ```
  from requests import Requests, Session

  url = 'http://httpbin.org/post'
  data = {
      'name': 'germey'
  }
  headers = {
      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'
  }
  s = session()
  rep = Requests('POST', url, data=data, headers=headers)
  p = s.prepare_requests(rep)   #转换为一个prepared request对象
  r = s.send(p)                 #send方法发送
  ```



### requests-html

#### 1.导入使用该库

```
from requests_html import HTMLSession

session = HTMLSession()
r = session.get('https://python.org/')
```

#### 2.查找所有链接

- 一般形式

  `r.html.links`


- 绝对路径形式

  `r.html.absolute_links`

#### 3.使用CSS选择器

```
# css选择器，id=downloads的节点的文本
Downloads = r.html.find('#downloads', first=True).text
print(Downloads)

>>>
Downloads
All releases
Source code
Windows
Mac OS X
Other Platforms
License
Alternative Implementations
```



### 正则表达式

#### 正则实例

``` HTML
html = '''<div id="songs-list">
    <h2 class="title">经典老歌</h2>
    <p class="introduction">
        经典老歌列表
    </p>
    <ul id="list" class="list-group">
        <li data-view="2">一路上有你</li>
        <li data-view="7">
            <a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
        </li>
        <li data-view="4" class="active">
            <a href="/3.mp3" singer="齐秦">往事随风</a>
        </li>
        <li data-view="6"><a href="/4.mp3" singer="beyond">光辉岁月</a></li>
        <li data-view="5"><a href="/5.mp3" singer="陈慧琳">记事本</a></li>
        <li data-view="5">
            <a href="/6.mp3" singer="邓丽君"><i class="fa fa-user"></i>但愿人长久</a>
        </li>
    </ul>
</div>'''
```

- 匹配class为active的li节点内部的歌手名和歌名

- 该正则解释：

  - 以li开头
  - .*？匹配通用字符（非贪婪匹配）
  - 接着找到active
  - 然后提取singer属性值
  - 最后匹配a节点文本，a节点最右侧是>，最左侧是</a>

  ```
  pattern = "<li.*?active.*?singer="(.*?)">(.*?)</a>"
  result = re.search(pattern, html, re.s)     #re.s匹配换行符
  if result:
  	print(result.group(1), result.group(2))
  ```

  ​

#### 1.match()

- 第一个参数：正则，第二个参数：需要匹配的字符串


- 从字符串起始位置匹配
- 成功返回结果，失败返回none
- match中的两个方法：
  - group()返回内容
  - span()结果字符串在原字符串中的位置

#### 2.search()

- 搜索整个被匹配字符串，返回第一个符合规则的字符串
- 没有找到返回none

#### 3.findall()

- 搜索整个字符串，匹配所有符合规则的内容
- 返回值类型为列表

#### 4.sub()

- 修改文本

- 实例去掉字符串中的所有数字

  ```
  import re

  content = '6d67d6d6d7d6d76'
  content = re.sub('\d+', '', content)
  print(content)
  ```

#### 5.compile()

- 将正则编译成一个对象

- 编译后可以复用

  ```
  # 去掉时钟
  content = '2018-04-25 12:00'
  pattern = re.compile('\d{2}:\d{\2}')
  result = re.sub(pattern, '', content)
  ```

## 第二章 解析库

### xpath

#### 1.实例

- 定义一段html文本

- 最后一个li标签是没有闭合的

- ​

  ``` python
  from lxml import etree
  text = '''
  <div>
      <ul>
           <li class="item-0"><a href="link1.html">first item</a></li>
           <li class="item-1"><a href="link2.html">second item</a></li>
           <li class="item-inactive"><a href="link3.html">third item</a></li>
           <li class="item-1"><a href="link4.html">fourth item</a></li>
           <li class="item-0"><a href="link5.html">fifth item</a>
       </ul>
   </div>
  '''
  html = etree.HTML(text)        #初始化
  result = etree.tostring(html)  #修正html代码
  print(result.decode('utf-8'))  #byte转str
  ```

#### 2.选取所有节点

- //*
- 返回列表

#### 3.子节点

- 选择li节点下的所有a节点------`//li/a`
- /  获取直接子节点
- //  获取子孙节点

#### 4.父节点

- 获取href=link4的父节点class属性-------`//a[@href="link4.html"]/../@class`
  - 也可以写成-------`//a[@href="link4.html"]/parent::*/@class`

#### 5.属性匹配

- 用@符号进行属性过滤
- 选取class为item-0的li节点-----`//li[@class="item-0"]`

#### 6.文本获取

- 使用test()获取文本


- 获取class为item-0的li节点的文本
- 获取指定节点文本----`//li[@href="link4.html"]/a/text()`
- 获取子孙节点内部所有文本----`//li[@href="link4.html"]//text()`

#### 7.属性获取

- 获取li节点下所有a节点的href属性
- `//li/a/@href`
- 返回列表

#### 8.多属性匹配

- 如果一个节点的属性具有多个值，需要用contain()函数

- 第一个参数传入属性名称，第二个参数传入属性值

  ```
  <li class="li li-first"><a href="link.html">first item</a></li>
  # 如上 li节点的class属性有两个值
  # 获取该属性值则需要用contain函数
  //li[contains(@class,"li")]/a/text()
  ```

#### 9.按序选择

- 括号传入索引获取特定次序的节点
- 选取第一个li节点----`//li[1]/a/text()`
  - 注意，这里的索引值从1开始，并非从0开始


- 选取最后一个li节点-----`//li[last()]/a/text()`
- 选取位置小于3的li节点-----`//li[position()<3]/a/text()`
- 选取倒数第二个li节点-----`//li[last()-2]/a/text()`

### BeautifulSoup

#### 1.BeautifulSoup初始化

- `soup = BeautifulSoup(html, 'lxml')`
- 第一个参数为需要解析的html，第二个为指定lxml解析器

#### 2.格式化输出

- `soup.prettify()`
- 把需要解析的字符串已标准的格式缩进（实际在初始化时已经完成）

### PyQuery

## 第三章 数据存储

### TXT

### Json

### CSV

### MySQL(关系型)

#### 1.连接数据库

```
import pymysql

db = pymysql.connect(host='localhost', user='root', password='wo@NI123',
                     port=3306)
cursor = db.cursor()                 # 获取MySQL游标
cursor.execute('SELECT VERSION()')
data = cursor.fetchone()             # 获取执行SQL语句后的数据
print(data)
cursor.execute("CREATE DATABASE Alex DEFAULT CHARACTER SET utf8")
db.close()
```

#### 2.创建表

```
import pymysql

db = pymysql.connect(host='localhost', user='root', password='wo@NI123',
                     port=3306, db='alex')
cursor = db.cursor()                 # 获取MySQL游标
sql = 'CREATE TABLE IF NOT EXISTS students(id VARCHAR(255) NOT NULL, name VARCHAR (255) NOT NULL, age INT NOT NULL, PRIMARY KEY (id))'
cursor.execute(sql)
db.close()
```

#### 3.动态插入、更新数据

- 需要插入的数据为字典类型
- 动态构造字段名
- 动态构造占位符

``` python
import pymysql

db = pymysql.connect(host='localhost', user='root', password='wo@NI123',
                     port=3306, db='alex')
cursor = db.cursor()                 # 获取MySQL游标
data = {
    'id': '20180426',
    'name': 'liyhang',
    'age': '25'
}
table = 'students'
# 用逗号分隔开data里面的key值，再拼接起来
keys = ','.join(data.keys())
# 定义长度为1的数组乘以列表的元素个数，再利用format方法构造表名，字段名，占位符
values = ','.join(['%s'] * len(data))
sql = 'INSERT INTO {table}({keys}) VALUES 			({values}) ON DUPLICATE KEY UPDATE'.format(table=table,keys=keys, values=values)
update = ','.join(["{key} = %s".format(key=key) for key in data])
sql += update
try:
    if cursor.execute(sql, tuple(data.values())*2):
        print('ok')
        db.commit()
except:
    print('faild')
    db.rollback()
db.close()
```

#### 4.删除数据

```
import pymysql
...

table = 'students'
condition = 'age = 20'

sql = 'DELETE FROM {table} WHERE {condition}.format(table=table, condition=condition)
try:
	cursor.execute(sql)
	db.commit()
except:
	db.rollback()
db.close()

```

#### 5.查询数据

```
sql = 'SELECT * FROM student WHERE age >= 20'
try:
	cursor.execute(sql)
	print('Count:', cursor.rowcount)
	row = cursor.fetchone()
	while row:
		print('Row:', row)
		row = cursor.fetchone()
except:
	print('Error')
```



### MongoDB

### Redis





## 练习

#### 1.生活大爆炸贴吧爬取

- 爬取帖子标题、时间等

- 失败，原因未明

  ```
  import requests
  from bs4 import BeautifulSoup

  url = "http://tieba.baidu.com/f?kw=%E7%94%9F%E6%B4%BB%E5%A4%A7%E7%88%86%E7%82%B8&ie=utf-8"
  headers = {
          "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.5083.400 QQBrowser/10.0.972.400"
      }
  r = requests.get(url=url, headers=headers,timeout=30)
  r.raise_for_status()
  r.encoding = 'utf-8'
  html = r.text

  comments = []

  soup = BeautifulSoup(html, 'lxml')

  liTags = soup.find_all(class_='j_thread_list')
  print(liTags)

  for li in liTags:
      comment = {}
      comment['title'] = li.find('a', attrs={'class': 'j_th_tit'}).text.strip()
      comment['link'] = "http://tieba.baidu.com/" + li.find('a', attrs={'class': 'j_th_tit '})['href']
      comment['name'] = li.find('span', attrs={'class': 'tb_icon_author '}).text.strip()
      comment['time'] = li.find('span', attrs={'class': 'pull-right is_show_create_time'}).text.strip()
      comment['replyNum'] = li.find('span', attrs={'class': 'threadlist_rep_num center_text'}).text.strip()
      comments.append(comment)
      print(comment)
  print(comments)
  ```

  ​